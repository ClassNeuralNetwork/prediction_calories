import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Carregar os dados
input_train = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/train/input_train.csv')
output_train = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/train/output_train.csv')
input_test = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/test/input_test.csv')
output_test = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/test/output_test.csv')

# Normalizar os dados
scaler_input = StandardScaler()
i_train = scaler_input.fit_transform(input_train)
i_test = scaler_input.transform(input_test)

scaler_output = StandardScaler()
o_train = scaler_output.fit_transform(output_train.values.reshape(-1, 1))
o_test = scaler_output.transform(output_test.values.reshape(-1, 1))

# Definir o modelo
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(i_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='linear')  # Saída única para regressão
])

# Configurar early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)

# Compilar o modelo
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Resumir o modelo
model.summary()

# Treinar o modelo
history = model.fit(i_train, o_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])

# Avaliar o modelo no conjunto de teste
test_loss, test_mae = model.evaluate(i_test, o_test)
print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')

# Fazer previsões no conjunto de teste
predicoes = model.predict(i_test)

# Inverter a normalização das previsões
predicoes_inverted = scaler_output.inverse_transform(predicoes)
output_test_inverted = scaler_output.inverse_transform(o_test)

# Exibir previsões
print(predicoes_inverted[:10])
print(output_test_inverted[:10])

# Plotar a perda durante o treinamento
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.yscale('log')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Comparar valores reais e previstos
plt.scatter(output_test_inverted, predicoes_inverted, alpha=0.5)
plt.plot([output_test_inverted.min(), output_test_inverted.max()], [output_test_inverted.min(), output_test_inverted.max()], 'r--')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('True vs Predicted Values')
plt.show()

=========================================================================================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import tensorflow as tf

# Carregar os dados
input_train = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/train/input_train.csv')
output_train = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/train/output_train.csv')

input_test = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/test/input_test.csv')
output_test = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/test/output_test.csv')

# Normalizar os dados
scaler_input = StandardScaler()
i_train = scaler_input.fit_transform(input_train)
i_test = scaler_input.transform(input_test)

scaler_output = StandardScaler()
o_train = scaler_output.fit_transform(output_train.values.reshape(-1, 1))
o_test = scaler_output.transform(output_test.values.reshape(-1, 1))

# Definir o modelo
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(i_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='linear')  # Saída única para regressão
])

# Configurar early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)

# Compilar o modelo
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Resumir o modelo
model.summary()

# Treinar o modelo
history = model.fit(i_train, o_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])

# Avaliar o modelo no conjunto de teste
test_loss, test_mae = model.evaluate(i_test, o_test)
print(f'Test Loss: {test_loss}, Test MAE: {test_mae}')

# Fazer previsões no conjunto de teste
predicoes = model.predict(i_test)

# Inverter a normalização das previsões
predicoes_inverted = scaler_output.inverse_transform(predicoes)
output_test_inverted = scaler_output.inverse_transform(o_test)

# Exibir previsões
print(predicoes_inverted[:10])
print(output_test_inverted[:10])

# Plotar a perda durante o treinamento
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.yscale('log')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.show()

# Comparar valores reais e previstos
plt.scatter(output_test_inverted, predicoes_inverted, alpha=0.5)
plt.plot([output_test_inverted.min(), output_test_inverted.max()], [output_test_inverted.min(), output_test_inverted.max()], 'r--')
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('True vs Predicted Values')
plt.show()

================================================================================================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
import shap

# Carregar os dados
input_train = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/train/input_train.csv')
output_train = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/train/output_train.csv')
input_test = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/test/input_test.csv')
output_test = pd.read_csv('C:/project_topicos_especiais/prediction_calories/prediction_calories/dataset/test/output_test.csv')

# Verificar a forma dos dados
print("input_train shape:", input_train.shape)
print("output_train shape:", output_train.shape)
print("input_test shape:", input_test.shape)
print("output_test shape:", output_test.shape)

# Definir o modelo
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(input_train.shape[1],)))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(1, activation='linear'))  # Ajustar unidades conforme a saída

# Configurar early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)

# Compilar o modelo
model.compile(optimizer='adam', loss='mse')

# Resumir o modelo
model.summary()

# Treinar o modelo
history = model.fit(input_train, output_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])

# Fazer previsões
predicoes = model.predict(input_test)

# Se necessário, aplicar inversão de normalização
scaler_output = StandardScaler()
output_test_scaled = scaler_output.fit_transform(output_test.values.reshape(-1, 1))
predicoes_inverted = scaler_output.inverse_transform(predicoes)

# Exibir previsões
print(predicoes_inverted)

# Plotar o histórico de treinamento
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.yscale('log')
plt.show()

# SHAP para explicabilidade
#explainer = shap.KernelExplainer(model.predict, input_test)
#shap_values = explainer.shap_values(input_test)

#shap.summary_plot(shap_values, input_test)
